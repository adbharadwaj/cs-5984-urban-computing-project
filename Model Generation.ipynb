{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.datasets.species_distributions import construct_grids\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def parse_location(loc):\n",
    "    loc = loc.strip(\"()\").split(',')\n",
    "    lat = loc[0].strip()\n",
    "    long = loc[1].strip()\n",
    "    return float(lat), float(long)\n",
    "\n",
    "def classification_report(y_true, y_pred, verbose=False):\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    accuracy_score = metrics.accuracy_score(y_true, y_pred)\n",
    "    recall_score = metrics.recall_score(y_true, y_pred)\n",
    "    precision_score = metrics.precision_score(y_true, y_pred)\n",
    "    f1_score = metrics.f1_score(y_true, y_pred)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    if verbose:\n",
    "        print('confusion_matrix')\n",
    "        print(confusion_matrix)\n",
    "        print('accuracy_score', accuracy_score)\n",
    "        print('recall_score', recall_score)\n",
    "        print('precision_score', precision_score)\n",
    "        print('f1_score', f1_score)\n",
    "        print('auc', auc)\n",
    "        print('null_accuracy', (len(y_true)-sum(y_true))/len(y_true))\n",
    "    \n",
    "    return confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, auc\n",
    "    \n",
    "\n",
    "def get_scaler(column):\n",
    "    X = np.array(column).reshape(column.shape[0], 1)\n",
    "    return preprocessing.MinMaxScaler().fit(column)\n",
    "\n",
    "def scale(scaler, column):\n",
    "    X = np.array(column).reshape(column.shape[0], 1)\n",
    "    return scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 s, sys: 72 ms, total: 2.08 s\n",
      "Wall time: 2.07 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_range</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>crime_freq</th>\n",
       "      <th>yelp_freq</th>\n",
       "      <th>police_freq</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>police_factor</th>\n",
       "      <th>yelp_factor</th>\n",
       "      <th>prev_7_days_crime_freq</th>\n",
       "      <th>prev_day_crime_freq</th>\n",
       "      <th>PRCP</th>\n",
       "      <th>SNOW</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((41.5487, -88.3713), (41.560078, -88.345754))</td>\n",
       "      <td>2006-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.554389</td>\n",
       "      <td>-88.358527</td>\n",
       "      <td>7.041843e-26</td>\n",
       "      <td>0.019742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>-3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((41.5487, -88.3713), (41.560078, -88.345754))</td>\n",
       "      <td>2006-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.554389</td>\n",
       "      <td>-88.358527</td>\n",
       "      <td>7.041843e-26</td>\n",
       "      <td>0.019742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((41.5487, -88.3713), (41.560078, -88.345754))</td>\n",
       "      <td>2006-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.554389</td>\n",
       "      <td>-88.358527</td>\n",
       "      <td>7.041843e-26</td>\n",
       "      <td>0.019742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((41.5487, -88.3713), (41.560078, -88.345754))</td>\n",
       "      <td>2006-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.554389</td>\n",
       "      <td>-88.358527</td>\n",
       "      <td>7.041843e-26</td>\n",
       "      <td>0.019742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((41.5487, -88.3713), (41.560078, -88.345754))</td>\n",
       "      <td>2006-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41.554389</td>\n",
       "      <td>-88.358527</td>\n",
       "      <td>7.041843e-26</td>\n",
       "      <td>0.019742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               cell_range  timestamp  \\\n",
       "timestamp                                                              \n",
       "1          ((41.5487, -88.3713), (41.560078, -88.345754)) 2006-01-01   \n",
       "1          ((41.5487, -88.3713), (41.560078, -88.345754)) 2006-01-02   \n",
       "1          ((41.5487, -88.3713), (41.560078, -88.345754)) 2006-01-03   \n",
       "1          ((41.5487, -88.3713), (41.560078, -88.345754)) 2006-01-04   \n",
       "1          ((41.5487, -88.3713), (41.560078, -88.345754)) 2006-01-05   \n",
       "\n",
       "           crime_freq  yelp_freq  police_freq        lat       long  \\\n",
       "timestamp                                                             \n",
       "1                   0          0            0  41.554389 -88.358527   \n",
       "1                   0          0            0  41.554389 -88.358527   \n",
       "1                   0          0            0  41.554389 -88.358527   \n",
       "1                   0          0            0  41.554389 -88.358527   \n",
       "1                   0          0            0  41.554389 -88.358527   \n",
       "\n",
       "           police_factor  yelp_factor  prev_7_days_crime_freq  \\\n",
       "timestamp                                                       \n",
       "1           7.041843e-26     0.019742                     NaN   \n",
       "1           7.041843e-26     0.019742                     NaN   \n",
       "1           7.041843e-26     0.019742                     NaN   \n",
       "1           7.041843e-26     0.019742                     NaN   \n",
       "1           7.041843e-26     0.019742                     NaN   \n",
       "\n",
       "           prev_day_crime_freq  PRCP  SNOW  TMAX  TMIN  \n",
       "timestamp                                               \n",
       "1                          NaN   0.8     0   7.2  -3.3  \n",
       "1                            0   9.4     0   5.6   4.4  \n",
       "1                            0   0.0     0   5.6   3.9  \n",
       "1                            0   0.5     0   6.1   2.2  \n",
       "1                            0   0.0     0   2.8  -0.6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df = pd.read_csv('features_temporal_full_year_with_weather_2500_final.tsv', sep='\\t', parse_dates=['timestamp'])\n",
    "df.index = df.timestamp.apply(lambda x: x.month)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1] TEST: [2]\n",
      "TRAIN: [1 2] TEST: [3]\n",
      "TRAIN: [1 2 3] TEST: [4]\n",
      "TRAIN: [1 2 3 4] TEST: [5]\n",
      "TRAIN: [1 2 3 4 5] TEST: [6]\n",
      "TRAIN: [1 2 3 4 5 6] TEST: [7]\n",
      "TRAIN: [1 2 3 4 5 6 7] TEST: [8]\n",
      "TRAIN: [1 2 3 4 5 6 7 8] TEST: [9]\n",
      "TRAIN: [1 2 3 4 5 6 7 8 9] TEST: [10]\n",
      "TRAIN: [ 1  2  3  4  5  6  7  8  9 10] TEST: [11]\n",
      "TRAIN: [ 1  2  3  4  5  6  7  8  9 10 11] TEST: [12]\n"
     ]
    }
   ],
   "source": [
    "X = df\n",
    "y = df.crime_freq.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=11)\n",
    "\n",
    "frames = []\n",
    "for train_index, test_index in tscv.split(X.index.unique()):\n",
    "    print(\"TRAIN:\", train_index+1, \"TEST:\", test_index+1)\n",
    "#     X_train, X_test = X[X.index.isin(train_index+1)], X[X.index.isin(test_index+1)]\n",
    "#     y_train, y_test = y[y.index.isin(train_index+1)], y[y.index.isin(test_index+1)]\n",
    "#         print(X_train.timestamp.values[0], X_train.timestamp.values[-1], X_test.timestamp.values[0], X_test.timestamp.values[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Test Defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _classifier_name(classifier):\n",
    "    \n",
    "    name = type(classifier).__name__\n",
    "    if name == 'SVC':\n",
    "        return classifier.kernel + ' ' + name\n",
    "    return name\n",
    "    \n",
    "def train_and_test(X_train, X_test, y_train, y_test, features=None, verbose=False, random_state = 4, classifiers=None):\n",
    "    def scale_columns(train, test, column_names):\n",
    "        for column_name in column_names:\n",
    "            scaler = get_scaler(train[column_name])\n",
    "            train[column_name] = scale(scaler, train[column_name])\n",
    "            test[column_name] = scale(scaler, test[column_name])\n",
    "    \n",
    "    scale_columns(X_train, X_test, features)\n",
    "    \n",
    "    test = X_test.copy() \n",
    "    test['label'] = y_test\n",
    "    \n",
    "    X_train = X_train[features]\n",
    "    X_test = X_test[features]\n",
    "    \n",
    "    # class_weight = {\n",
    "    #     1: 10,\n",
    "    #     0: 1\n",
    "    # }\n",
    "    training_size = len(X_train.index.unique())\n",
    "    \n",
    "    class_weight = \"balanced\"\n",
    "    \n",
    "    if classifiers is None:\n",
    "        classifiers = [\n",
    "            DecisionTreeClassifier(max_depth=5, class_weight=class_weight, random_state=random_state),\n",
    "            RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, class_weight=class_weight, random_state=random_state),\n",
    "        ]\n",
    "    report = []\n",
    "    for classifier in classifiers:\n",
    "        if verbose:\n",
    "            print('\\n')\n",
    "            print('#'*10, type(classifier).__name__, '#'*10)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_test_pred = classifier.predict(X_test)\n",
    "        \n",
    "        test['pred_label'] = y_test_pred\n",
    "        test.to_csv('predictions_full_year_2500_v2/%s_%s.tsv' % (_classifier_name(classifier), training_size), sep='\\t', index=False)\n",
    "        test[test.label == 1][['lat', 'long']].to_csv('predictions_full_year_2500_v2/Locations/%s_true_crime_locations.csv' % _classifier_name(classifier), index=False)\n",
    "        test[test.pred_label == 1][['lat', 'long']].to_csv('predictions_full_year_2500_v2/Locations/%s_pred_crime_locations.csv' % _classifier_name(classifier), index=False)\n",
    "        \n",
    "        confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, auc = classification_report(y_test, y_test_pred, verbose=verbose)\n",
    "        report.append([_classifier_name(classifier), accuracy_score, recall_score, precision_score, f1_score, auc])\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time series split using cross validation time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_models(features, splits=2, verbose=False, classifiers=None):\n",
    "    if verbose:\n",
    "        print()\n",
    "        print('#'*100)\n",
    "        print('#'*10 + str(features) + '#'*10)\n",
    "        print('#'*100)\n",
    "        print()\n",
    "\n",
    "    X = df\n",
    "    y = df.crime_freq.apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=splits)\n",
    "    \n",
    "    frames = []\n",
    "    for train_index, test_index in tscv.split(X.index.unique()):\n",
    "#         print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[X.index.isin(train_index+1)], X[X.index.isin(test_index+1)]\n",
    "        y_train, y_test = y[y.index.isin(train_index+1)], y[y.index.isin(test_index+1)]\n",
    "#         print(X_train.timestamp.values[0], X_train.timestamp.values[-1], X_test.timestamp.values[0], X_test.timestamp.values[-1])\n",
    "    \n",
    "        train_start_date, train_end_date = X_train.timestamp.values[0], X_train.timestamp.values[-1]\n",
    "        test_start_date, test_end_date = X_test.timestamp.values[0], X_test.timestamp.values[-1]\n",
    "        training_size = len(X_train.index.unique())\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "            print('>'*100)\n",
    "            print(\"Training: \", str(train_start_date), \"to\" , str(train_end_date))\n",
    "            print(\"Test: \", str(test_start_date), \"to\" , str(test_end_date))\n",
    "            print('>'*100)\n",
    "            print()\n",
    "        \n",
    "        report = train_and_test(X_train, X_test, y_train, y_test, features=features, verbose=verbose, classifiers=classifiers)\n",
    "        report = [classifier_report+[train_start_date, train_end_date, test_start_date, test_end_date, training_size] for classifier_report in report]\n",
    "            \n",
    "        frames.extend(report)\n",
    "        \n",
    "    return pd.DataFrame(frames, columns=['classifier', \n",
    "                                             'accuracy_score', \n",
    "                                             'recall_score', \n",
    "                                             'precision_score', \n",
    "                                             'f1_score', \n",
    "                                             'auc', \n",
    "                                             'train_start_date', \n",
    "                                             'train_end_date', 'test_start_date', 'test_end_date', 'training_size'\n",
    "                                            ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################################################################\n",
      "##########['police_factor', 'yelp_factor', 'prev_day_crime_freq', 'prev_7_days_crime_freq']##########\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Training:  2006-01-08T00:00:00.000000000 to 2006-01-31T00:00:00.000000000\n",
      "Test:  2006-02-01T00:00:00.000000000 to 2006-02-28T00:00:00.000000000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "\n",
      "\n",
      "########## KNeighborsClassifier ##########\n",
      "confusion_matrix\n",
      "[[63880   528]\n",
      " [  405  5187]]\n",
      "accuracy_score 0.986671428571\n",
      "recall_score 0.927575107296\n",
      "precision_score 0.907611548556\n",
      "f1_score 0.917484743964\n",
      "auc 0.959688683942\n",
      "null_accuracy 0.920114285714\n",
      "\n",
      "\n",
      "########## SVC ##########\n",
      "confusion_matrix\n",
      "[[63675   733]\n",
      " [  171  5421]]\n",
      "accuracy_score 0.987085714286\n",
      "recall_score 0.969420600858\n",
      "precision_score 0.880890477738\n",
      "f1_score 0.923037629831\n",
      "auc 0.979020013508\n",
      "null_accuracy 0.920114285714\n",
      "\n",
      "\n",
      "########## DecisionTreeClassifier ##########\n",
      "confusion_matrix\n",
      "[[63178  1230]\n",
      " [   48  5544]]\n",
      "accuracy_score 0.981742857143\n",
      "recall_score 0.991416309013\n",
      "precision_score 0.818423383525\n",
      "f1_score 0.896652110626\n",
      "auc 0.986159651215\n",
      "null_accuracy 0.920114285714\n",
      "\n",
      "\n",
      "########## RandomForestClassifier ##########\n",
      "confusion_matrix\n",
      "[[63155  1253]\n",
      " [   44  5548]]\n",
      "accuracy_score 0.981471428571\n",
      "recall_score 0.992131616595\n",
      "precision_score 0.815762387884\n",
      "f1_score 0.895344145889\n",
      "auc 0.986338755758\n",
      "null_accuracy 0.920114285714\n",
      "\n",
      "\n",
      "########## LogisticRegression ##########\n",
      "confusion_matrix\n",
      "[[63425   983]\n",
      " [  219  5373]]\n",
      "accuracy_score 0.982828571429\n",
      "recall_score 0.960836909871\n",
      "precision_score 0.845342983008\n",
      "f1_score 0.899397388684\n",
      "auc 0.972787415313\n",
      "null_accuracy 0.920114285714\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Training:  2006-01-08T00:00:00.000000000 to 2006-02-28T00:00:00.000000000\n",
      "Test:  2006-03-01T00:00:00.000000000 to 2006-03-31T00:00:00.000000000\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\n",
      "\n",
      "\n",
      "########## KNeighborsClassifier ##########\n",
      "confusion_matrix"
     ]
    }
   ],
   "source": [
    "class_weight = \"balanced\"\n",
    "random_state = 4\n",
    "\n",
    "classifiers = [\n",
    "        KNeighborsClassifier(n_neighbors=5),\n",
    "#         SVC(kernel=\"linear\", C=0.025, class_weight=class_weight, random_state=random_state),\n",
    "        SVC(gamma=2, C=1, class_weight=class_weight, random_state=random_state),\n",
    "#         GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
    "        DecisionTreeClassifier(max_depth=5, class_weight=class_weight, random_state=random_state),\n",
    "        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, class_weight=class_weight, random_state=random_state),\n",
    "#         MLPClassifier(alpha=1, random_state=random_state),\n",
    "#         AdaBoostClassifier(random_state=random_state),\n",
    "#         GaussianNB(),\n",
    "#         QuadraticDiscriminantAnalysis(),\n",
    "        LogisticRegression(C=1e5, class_weight=class_weight, random_state=random_state)\n",
    "        ]\n",
    "\n",
    "features = [\n",
    "#     'PRCP', \n",
    "#     'SNOW', \n",
    "#     'TMAX', \n",
    "#     'TMIN',\n",
    "#     'crime_freq', \n",
    "#      'yelp_freq', \n",
    "#      'police_freq', \n",
    "     'police_factor', \n",
    "#     'crime_factor', \n",
    "    'yelp_factor', \n",
    "    'prev_day_crime_freq',\n",
    "    'prev_7_days_crime_freq'\n",
    "]\n",
    "\n",
    "report_df = test_models(features, classifiers=classifiers, splits=11, verbose=True)\n",
    "ax = report_df[['classifier', \n",
    "            'accuracy_score', \n",
    "            'recall_score', \n",
    "            'precision_score', \n",
    "            'f1_score', \n",
    "            'auc']].groupby(by=['classifier']).mean().plot(rot=90)\n",
    "\n",
    "for classifier in report_df.classifier.unique():\n",
    "    ax = report_df[report_df.classifier == classifier].groupby(by=['training_size']).mean().plot(title=classifier)\n",
    "    ax.xaxis.set_ticks(report_df.training_size.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report_df\n",
    "# report_df.to_csv('Jan_2_split_classification_report.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
